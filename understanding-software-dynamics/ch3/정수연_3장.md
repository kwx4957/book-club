# Understanding Software Dynamics Ch.03

3장 메모리 측정

## 책 내용 정리 (희망하는 사람만)
```text
p.75
많은 설계 계층은 서로 상호작용해 메모리 접근 패턴을 나타내며, 오래 지연되는 데이터 전송을 수행한다. 이 계층은 아래 항목을 포함한다.

- C 프로그래머
- 컴파일러
- 어셈블리 언어
- CPU 명령어
- 가상 메모리
- 캐시 메모리의 여러 계층
- 메인 메모리인 DRAM
```

## 좋았던 부분
```text
p.80
하나의 큰 캐시 계층 모두가 속도는 충분히 빠르고 공간은 충분히 커서 코어 간 공유될 수 있다면 가장 좋다.
하지만 불행히도 반도체 메모리 접근 시간은 메모리 크기가 커질수록 비례해서 늘어나기에 이는 불가능하다.
따라서 최신 캐시 설계는 메모리 접근 패턴이 도일한 위치에 접근하는 경우 비교적 낮은 비용으로 고성능을 제공할 수 있다.
시간이 지나 세부사항은 바뀔지라도 전체적인 개념은 변치 않을 것이다.

...

캐시 메모리는 몇 바이트의 라인 또는 블로긍로 구성되면 라인의 모든 바이트는 모두 함께꺼내 추가된다.
각 캐시 라인에는 해당 데이터의 메인 메모리 주소를 지정하는 태그가 존재한다.
캐시에서 데이터를 찾으려면 캐시 하드웨어는 메모리 주소를 하나 이상의 태그와 비교한다.

일치된 항목을 찾으면 이를 캐시 히트 cache hit 라고 부르며
캐시 히트가 되면 데이터에 빠르게 접근할 수 있다.

반면 일치된 항목이 없는 겻은 캐시 미스 cache miss 라고 부르고 이때는 메모리 계층의 하위 계층부터 더 천천히 접근해야 한다.

캐시 라인마다 필요한 데이터의 최소량은 2워드이며 워드는 대부분 포인터의 크기다. 
```

```text
p.84
일반적으로 주소 맵핑과 접근 시간은 L1 집한 연관 캐시 크기가 페이지 크기 * 연관도 associactivity 이하일 때만 겹치게 된다.
이 말은 4KB 정도의 작은 페이지 크기에는 작은 L1 캐시가 필요하다는 의미다.

결국 현업에서는 작은 페이지에 맵핑되지 않는 비트가 더 많아지기 때문에
더 큰 L1 캐시를 사용할 수 있도록 큰 페이지를 사용해야한다.

256GB 메인 메모리 RAM을 사용하는 데이터 센터 시스템에서 4KB 페이지로 관리하면 6400만 페이지를 의미하기에 이 페이지 수는 너무 많다.
따라서 더 큰 페이지 크기를 사용해 페이지 수를 줄이는 것이 더 빠른 속도를 낼 수 있는 방법이다.
```

```text
p.87
현대의 캐시는 종종 N 번 라인에 접근할 때 N+1 라인을 프리패치 하는 방법으로 첫 번째 캐시 미스를 제외한 나머지 캐시 미스는 제거한다.
또한 현대의 CPU는 비순차적인 실행 으로 CPU 사이클마다 여러 명령어를 실행하며 메모리 응답을 하도록 5~50 개의 아직 끝나지 않은 로드를 병렬로 대기시킨다.
그 결과 많은 캐시 미스가 발생하는 시간이 단 한번의 캐시 미스 시간과 거의 같아 전체 캐시 미스 시간을 10배 이상 줄일 수 있다.
```

```text
p.92 [3.11 캐시 계층별 크기 측정]
캐시 각 계층의 크기를 찾는 전략은 캐시에서 N 바이트를 읽은 다음 타이밍을 보고 다시 읽는 것이다.
...
실제 장비와 운영체제에서의 타이밍이 정확히 반복되는 경우는 거의 없다.
외부 인터럽트 external interrupts, 네트워크 트래픽, 사람의 입력과 브라우저나 디스플레이 소프트웨어와 같은 백그라운드 프로그램은
모두 동일한 프로그램의 다중 실행에서의 작은 시간 변화에 영향을 준다.

따라서 타이밍 데이터는 항상 약간의 노이즈가 있을 것이다.
또한 인접한 메모리 계층의 시간은 상대적으로 차이가 적기 때문에 타이밍을 계산하는 것은 쉽지 않다.
```

## 새로 배운 내용

## 책의 내용과 관련한 본인의 경험 (실무 경험, 프로젝트 경험)

## 데모 시연 (준비한 사람만) or 코드 조각

## 내용을 복습하기 위한 유튜브 링크나 논문 혹은 참고 자료들

## 이해 안되거나 모르는 부분

## 그외 자유롭게 작성